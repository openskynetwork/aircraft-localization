{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aircraft Localization Competition\n",
    "This notebook compile all the algorithms used for the competition.\n",
    "Some functions are using multiprocessing which has only be tested on Linux OS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import lightgbm as lgb\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import itertools\n",
    "import multiprocessing as mp\n",
    "import plotly.graph_objects as go\n",
    "import geopy\n",
    "from plotly.subplots import make_subplots\n",
    "from pandas.io.json import json_normalize\n",
    "import json\n",
    "from itertools import combinations\n",
    "import geopy.distance\n",
    "import pickle\n",
    "from scipy.optimize import fsolve, root\n",
    "from scipy.stats import iqr\n",
    "import warnings\n",
    "from scipy.interpolate import UnivariateSpline\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'OSN/competition/'\n",
    "training = pd.read_csv(path+'round1_competition.csv')\n",
    "sensors = pd.read_csv(path+'sensors.csv')\n",
    "validation = pd.read_csv(path+'round1_sample_empty.csv')\n",
    "testing = training.loc[training.id.isin(validation.id)]\n",
    "training = training.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Transformation\n",
    "\n",
    "## Json Expansion\n",
    "Here the goal is to split the measurements into multiple rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_measurements(df):\n",
    "    # Function to expand the json coumn called measurements\n",
    "\n",
    "    dfs = []\n",
    "    def json_to_df(row, json_col):\n",
    "        json_df = pd.read_json(row[json_col])\n",
    "        dfs.append(json_df.assign(**row.drop(json_col)))\n",
    "    df.apply(json_to_df, axis=1, json_col='measurements')\n",
    "    df = pd.concat(dfs).reset_index().rename(columns={0: 'sensor', 1:'timestamp', 2:'power'})\n",
    "   \n",
    "    return df\n",
    "\n",
    "def expand_measurements_para(df):\n",
    "    # Here we used multoprocessing to make things faster\n",
    "    num_processes = mp.cpu_count()\n",
    "    chunk_size = int((len(df)//num_processes)+1)\n",
    "    chunks = [df.iloc[i:i + chunk_size]for i in range(0, len(df), chunk_size)]\n",
    "\n",
    "    with mp.Pool(num_processes) as pool:\n",
    "        list_df = pool.map(expand_measurements, chunks)\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "    return pd.concat(list_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df_testing = pd.read_pickle('df_testing_clean.pkl')\n",
    "except:\n",
    "    df_testing = expand_measurements_para(testing)\n",
    "    df_testing.to_pickle('df_testing_clean.pkl')\n",
    "df_testing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df_training = pd.read_pickle('df_training_clean.pkl')\n",
    "except:\n",
    "    df_training = expand_measurements_para(training)\n",
    "    df_training.to_pickle('df_training_clean.pkl')\n",
    "\n",
    "df_training.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We only keep the sensors seen in the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training = df_training.loc[df_training.sensor.isin(df_testing.sensor.values)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get one row per pair of measurement to obtain the delta_time used in multilateration\n",
    "Here we need to obtain all the combinations of pairs of sensors available for one measurement and compute the difference in time of the received messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pairs_dt_n_power(df):\n",
    "    rows_list = []\n",
    "    for ids, group in df.groupby('id'):\n",
    "        sensor_pairs = [sorted(t) for t in list(combinations(group.sensor, 2))]\n",
    "        for pair in sensor_pairs:\n",
    "            dt_obs = group.loc[group.sensor==pair[0]].timestamp.values[0]-group.loc[group.sensor==pair[1]].timestamp.values[0]\n",
    "            p0 = group.loc[group.sensor==pair[0]].power.values[0]\n",
    "            p1 = group.loc[group.sensor==pair[1]].power.values[0]\n",
    "            tAtServer = group.loc[group.sensor==pair[1]].timeAtServer.values[0]\n",
    "            lat = group.latitude.values[0]\n",
    "            lon = group.longitude.values[0]\n",
    "            baro = group.baroAltitude.values[0]\n",
    "            geo =  group.geoAltitude.values[0]\n",
    "            rows_list.append({'id':ids, 's0':pair[0], 's1':pair[1], 'dt_obs':dt_obs,\n",
    "                              'p0': p0, 'p1': p1, 'timeAtServer': tAtServer,\n",
    "                              'latitude': lat, 'longitude': lon, 'baroAltitude': baro, 'geoAltitude': geo})\n",
    "    return pd.DataFrame(rows_list)\n",
    "\n",
    "def get_pairs_dt_n_power_para(df):\n",
    "    # Here we use multoprocessing to make things faster\n",
    "    num_processes = mp.cpu_count()\n",
    "    grouped = df.groupby('id')\n",
    "    list_groups = [g[1] for g in list(grouped)]\n",
    "    chunk_size = int((len(list_groups)//num_processes)+1)                      \n",
    "    chunks = [pd.concat(list_groups[i:i+chunk_size]) for i in range(0, len(list_groups), chunk_size)]\n",
    "\n",
    "    with mp.Pool(num_processes) as pool:\n",
    "        list_df = pool.map(get_pairs_dt_n_power, chunks)\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "    return pd.concat(list_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    X_test = pd.read_pickle('X_test_clean.pkl')\n",
    "except:\n",
    "    X_test = get_pairs_dt_n_power_para(df_testing)\n",
    "    X_test.to_pickle('X_test_clean.pkl')\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    X_train = pd.read_pickle('X_train_clean.pkl')\n",
    "except:\n",
    "    X_train = get_pairs_dt_n_power_para(df_training)\n",
    "    X_train.to_pickle('X_train_clean.pkl')\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute the theoretical dt for the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distance_diff(s0, s1, ac_pos):\n",
    "    s0 = tuple(s0.values[0])\n",
    "    s1 = tuple(s1.values[0])\n",
    "    ac_pos = tuple(ac_pos.values)\n",
    "    ds0_ac_m = np.sqrt(geopy.distance.distance(s0[:2], ac_pos[:2]).m**2 + (s0[2]-ac_pos[2])**2)\n",
    "    ds1_ac_m = np.sqrt(geopy.distance.distance(s1[:2], ac_pos[:2]).m**2 + (s1[2]-ac_pos[2])**2)\n",
    "\n",
    "    return ds0_ac_m-ds1_ac_m\n",
    "\n",
    "def get_dt_calc(df):\n",
    "    dt_calcs = []\n",
    "    c = 0.2995 # Speed of transmission in m/ns\n",
    "    for _, row in df.iterrows():\n",
    "        s0 = s0_lon, s0_lat, s0_alt = sensors.loc[sensors.serial==row.s0][['latitude', 'longitude', 'height']]\n",
    "        s1 = s1_lon, s1_lat, s1_alt = sensors.loc[sensors.serial==row.s1][['latitude', 'longitude', 'height']]\n",
    "        ac_pos = row[['latitude' , 'longitude', 'geoAltitude']]\n",
    "        doffset_m = get_distance_diff(s0, s1, ac_pos)\n",
    "        dt_calcs.append(doffset_m/c)\n",
    "    df['dt_calc'] = dt_calcs\n",
    "    return df\n",
    "\n",
    "def get_dt_calc_para(df, sens):\n",
    "    global sensors\n",
    "    sensors = sens\n",
    "    # Here we use multoprocessing to make things faster\n",
    "    num_processes = mp.cpu_count()\n",
    "    chunk_size = int((len(df)//num_processes)+1)                      \n",
    "    chunks = [df.iloc[i:i+chunk_size] for i in range(0, len(df), chunk_size)]\n",
    "\n",
    "    with mp.Pool(num_processes) as pool:\n",
    "        list_df = pool.map(get_dt_calc, chunks)\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "    return pd.concat(list_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    X_train_calc = pd.read_pickle('X_train_calc_clean.pkl')\n",
    "except:\n",
    "    X_train_calc = get_dt_calc_para(X_train, sensors)\n",
    "    X_train_calc.to_pickle('X_train_calc_clean.pkl')\n",
    "X_train = X_train_calc\n",
    "X_train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparaison between theory and obesrvations for each pair of sensors and offset correction\n",
    "Due to the fact that sensor clocks are supposed to be synchronized, it is surprising to see that for some pairs of sensors it looks like there is an offset. We will correct this using the median."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['dt_corrected'] = np.nan\n",
    "X_test['dt_corrected'] = np.nan\n",
    "for s, group in X_train.groupby(['s0','s1']):\n",
    "    diffs = group.dt_obs-group.dt_calc\n",
    "    print(s, np.quantile(diffs, 0.1), np.median(diffs), np.quantile(diffs, 0.9))\n",
    "    X_train.loc[group.index, 'dt_corrected'] = (group.dt_obs -np.median(diffs)).values\n",
    "    diffs2 = X_train.loc[group.index].dt_corrected-group.dt_calc\n",
    "    print(s, np.quantile(diffs2, 0.1), np.median(diffs2), np.quantile(diffs2, 0.9))\n",
    "    X_test.loc[(X_test.s0 == s[0]) & (X_test.s1 == s[1]), 'dt_corrected'] = X_test.loc[(X_test.s0 == s[0]) & (X_test.s1 == s[1])].dt_obs-np.median(diffs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the fiffrence between the corrected and the calculated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffs = X_train.dt_calc - X_train.dt_corrected\n",
    "px.histogram(diffs.sample(frac =.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see there are some HUGE outliers, we will try to remove thos ones by keeping only values which are closer to 1000ns of difference.  (1000ns corresponds to 300m difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_filtered = X_train.loc[np.abs(diffs)<1000]\n",
    "diffs = X_train_filtered.dt_calc - X_train_filtered.dt_corrected\n",
    "px.histogram(diffs.sample(frac =.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now it is filtered, we can do the same deoffset  again\n",
    "In case the median was still influenced by outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test['dt_corrected2'] = np.nan\n",
    "X_train['dt_corrected2'] = np.nan\n",
    "grouped = X_test.groupby(['s0','s1'])\n",
    "pairs = [g[0] for g in list(grouped)]\n",
    "# Then we compute the difference for each pairs\n",
    "for s0, s1 in pairs:\n",
    "    pair = (s0, s1)\n",
    "    X_train_pairs = X_train_filtered.loc[(X_train_filtered.s0==s0) & (X_train_filtered.s1==s1)]\n",
    "    if len(X_train_pairs)<1:\n",
    "        print('missed pair:', pair)\n",
    "        continue\n",
    "    diffs = X_train_pairs['dt_calc']-X_train_pairs['dt_corrected']\n",
    "    print('Pair:', (s0, s1), 'quantiles:', np.quantile(diffs, [0.1, 0.5, 0.9]), np.mean(diffs))\n",
    "    X_train_filtered.loc[(X_train_filtered.s0==s0) \n",
    "                         & (X_train_filtered.s1==s1),\n",
    "                         'dt_corrected2'] = X_train_filtered.loc[(X_train_filtered.s0==s0)\n",
    "                                                                  & (X_train_filtered.s1==s1)].dt_corrected+np.median(diffs)\n",
    "    X_test.loc[(X_test.s0==s0) & (X_test.s1==s1), 'dt_corrected2'] = X_test.loc[(X_test.s0==s0)\n",
    "                                                                         & (X_test.s1==s1)].dt_corrected+np.median(diffs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffs = X_train_filtered.dt_calc - X_train_filtered.dt_corrected2\n",
    "px.histogram(diffs.sample(frac =.05))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_filtered.to_pickle('X_train_filtered.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bingo This is nice now! The problem is that we have some pairs of sensors which have completly been ruled out fue to the filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test['dt_corrected2'] = X_test['dt_corrected2'].fillna(X_test['dt_corrected'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['s0', 's1', 'latitude', 'longitude', 'timeAtServer', 'p0', 'p1', 'baroAltitude', 'dt_obs', 'dt_corrected','dt_corrected2']\n",
    "X_training = X_train_filtered[features].dropna().copy()\n",
    "X_training[\"s0\"] = X_training[\"s0\"].astype('category')\n",
    "X_training[\"s1\"] = X_training[\"s1\"].astype('category')\n",
    "Y = X_train_filtered.dropna()['dt_calc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('RMSE baro/geo:', np.sqrt(mean_squared_error(Y, X_training.dt_corrected)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GeoAltitude Estimation\n",
    "We will use a light gradient boosting regression to predict the geoAltitude. We use the half / half methode where we train on 50% and test on the rest. Then we train a second model on the oposite datasets and at the end we combine the two results by taking the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['s0', 's1', 'timeAtServer', 'p0', 'p1', 'baroAltitude', 'dt_corrected2']\n",
    "\n",
    "X_training = X_train_filtered[features].copy()\n",
    "X_training[\"s0\"] = X_training[\"s0\"].astype('category')\n",
    "X_training[\"s1\"] = X_training[\"s1\"].astype('category')\n",
    "Y = X_train_filtered['geoAltitude']\n",
    "\n",
    "X_testing = X_test[features].copy()\n",
    "X_testing[\"s0\"] = X_testing[\"s0\"].astype('category')\n",
    "X_testing[\"s1\"] = X_testing[\"s1\"].astype('category')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the rmse between the geoAltitude and the barometric one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('RMSE baro/geo:', np.sqrt(mean_squared_error(Y, X_training.baroAltitude)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'regression',\n",
    "    'metric': 'rmse',\n",
    "    'max_depth': 10,\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 1,\n",
    "    'verbose': 0, \n",
    "    'early_stopping_round': 50,\n",
    "    }\n",
    "n_estimators = 100000\n",
    "\n",
    "# We will train 2 models each on one half of the data and sum it up at the end\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(X_training, Y, test_size=0.5, random_state=1)\n",
    "d_train = lgb.Dataset(x_train, label=y_train)\n",
    "d_valid = lgb.Dataset(x_valid, label=y_valid)\n",
    "watchlist = [d_valid]\n",
    "model1 = lgb.train(params, d_train, n_estimators, watchlist, verbose_eval=1)\n",
    "\n",
    "# We flip the training and testing\n",
    "x_train, x_valid = x_valid, x_train\n",
    "y_train, y_valid = y_valid, y_train\n",
    "\n",
    "# And we train the second model\n",
    "d_train = lgb.Dataset(x_train, label=y_train)\n",
    "d_valid = lgb.Dataset(x_valid, label=y_valid)\n",
    "watchlist = [d_valid]\n",
    "model2 = lgb.train(params, d_train, n_estimators, watchlist, verbose_eval=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preds = (model1.predict(X_training)+model2.predict(X_training))/2\n",
    "# X_train_filtered['geoAlt_pred'] = preds\n",
    "# preds = X_train_filtered.join(X_train_filtered[['id', 'geoAlt_pred']].groupby('id').median(), on='id', rsuffix='_med')\n",
    "# print('RMSE predict/geo:', np.sqrt(mean_squared_error(preds.geoAlt_pred_med, X_train_filtered.geoAlt_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test['geoAlt_pred'] = (model1.predict(X_testing) + model2.predict(X_testing)) / 2\n",
    "preds = X_test.join(X_test[['id', 'geoAlt_pred']].groupby('id').median(), on='id', rsuffix='_med')\n",
    "X_test['geoAltitude'] = preds.geoAlt_pred_med.values\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimating latitude / longitude using gradient boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['s0', 's1', 'timeAtServer', 'p0', 'p1', 'geoAltitude', 'dt_corrected2']\n",
    "\n",
    "X_training = X_train_filtered[features].copy()\n",
    "X_training[\"s0\"] = X_training[\"s0\"].astype('category')\n",
    "X_training[\"s1\"] = X_training[\"s1\"].astype('category')\n",
    "Y = X_train_filtered['latitude']\n",
    "\n",
    "X_testing = X_test[features].copy()\n",
    "X_testing[\"s0\"] = X_testing[\"s0\"].astype('category')\n",
    "X_testing[\"s1\"] = X_testing[\"s1\"].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'regression',\n",
    "    'metric': 'rmse',\n",
    "    'max_depth': 5,\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 1,\n",
    "    'verbose': 0, \n",
    "    'early_stopping_round': 50,\n",
    "    }\n",
    "n_estimators = 2000\n",
    "\n",
    "# We will train 2 models each on one half of the data and sum it up at the end\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(X_training, Y, test_size=0.5, random_state=1)\n",
    "d_train = lgb.Dataset(x_train, label=y_train)\n",
    "d_valid = lgb.Dataset(x_valid, label=y_valid)\n",
    "watchlist = [d_valid]\n",
    "model_lat1 = lgb.train(params, d_train, n_estimators, watchlist, verbose_eval=1)\n",
    "\n",
    "# We flip the training and testing\n",
    "x_train, x_valid = x_valid, x_train\n",
    "y_train, y_valid = y_valid, y_train\n",
    "\n",
    "# And we train the second model\n",
    "d_train = lgb.Dataset(x_train, label=y_train)\n",
    "d_valid = lgb.Dataset(x_valid, label=y_valid)\n",
    "watchlist = [d_valid]\n",
    "model_lat2 = lgb.train(params, d_train, n_estimators, watchlist, verbose_eval=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We check the results on the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = (model_lat1.predict(X_training)+model_lat2.predict(X_training))/2\n",
    "X_train_filtered['lat_pred'] = preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_lat = X_train_filtered[['id', 'latitude', 'lat_pred']].groupby('id').median()\n",
    "print('RMSE predict/geo:', np.sqrt(mean_squared_error(prediction_lat.latitude, prediction_lat.lat_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(prediction_lat['lat_pred']-prediction_lat['latitude'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's far from being good but it can be used for first guess during multilateration solving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We do the same for the longitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['s0', 's1', 'timeAtServer', 'p0', 'p1', 'geoAltitude', 'dt_corrected2']\n",
    "\n",
    "X_training = X_train_filtered[features].copy()\n",
    "X_training[\"s0\"] = X_training[\"s0\"].astype('category')\n",
    "X_training[\"s1\"] = X_training[\"s1\"].astype('category')\n",
    "Y = X_train_filtered['longitude']\n",
    "\n",
    "X_testing = X_test[features].copy()\n",
    "X_testing[\"s0\"] = X_testing[\"s0\"].astype('category')\n",
    "X_testing[\"s1\"] = X_testing[\"s1\"].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'regression',\n",
    "    'metric': 'rmse',\n",
    "    'max_depth': 5,\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 1,\n",
    "    'verbose': 0, \n",
    "    'early_stopping_round': 50,\n",
    "    }\n",
    "n_estimators = 2000\n",
    "\n",
    "# We will train 2 models each on one half of the data and sum it up at the end\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(X_training, Y, test_size=0.5, random_state=1)\n",
    "d_train = lgb.Dataset(x_train, label=y_train)\n",
    "d_valid = lgb.Dataset(x_valid, label=y_valid)\n",
    "watchlist = [d_valid]\n",
    "model_lon1 = lgb.train(params, d_train, n_estimators, watchlist, verbose_eval=1)\n",
    "\n",
    "# We flip the training and testing\n",
    "x_train, x_valid = x_valid, x_train\n",
    "y_train, y_valid = y_valid, y_train\n",
    "\n",
    "# And we train the second model\n",
    "d_train = lgb.Dataset(x_train, label=y_train)\n",
    "d_valid = lgb.Dataset(x_valid, label=y_valid)\n",
    "watchlist = [d_valid]\n",
    "model_lon2 = lgb.train(params, d_train, n_estimators, watchlist, verbose_eval=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We check the results on the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = (model_lon1.predict(X_training)+model_lon2.predict(X_training))/2\n",
    "X_train_filtered['lon_pred'] = preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_lon = X_train_filtered[['id', 'longitude', 'lon_pred']].groupby('id').median()\n",
    "print('RMSE predict/geo:', np.sqrt(mean_squared_error(prediction_lon.longitude, prediction_lon.lon_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(prediction_lon['lon_pred']-prediction_lon['longitude'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We predict the latitude and longitdude on the testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test['latitude'] = (model_lat1.predict(X_testing)+model_lat2.predict(X_testing))/2\n",
    "X_test['longitude'] = (model_lon1.predict(X_testing)+model_lon2.predict(X_testing))/2\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_lat_lon = X_test[['id', 'latitude', 'longitude']].groupby('id').median()\n",
    "prediction_lat_lon.head()\n",
    "testing[['latitude', 'longitude']] = prediction_lat_lon.values\n",
    "testing.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilateration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part we will apply multilateration to try to find aircraft positions.\n",
    "The strategy is to compute a solution for each triplet of sensors that belong to the same measurement.\n",
    "Ny doing so we can then see if all solutions are close or if some are outliers (in the modified z score sense). Maybe we can identify fauty sensor by doing so.\n",
    "Then we retrieve all the sensors which are not outliers in the sense of the modified z score and we solve the multilateration equation with all of them at once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To solve the equations, we need a first guess. We retriebe the lgbm results and we interpolate them with a polynom of order 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_guess = testing[['latitude', 'longitude', 'id']]\n",
    "df_guess['lat_interp'] = np.nan\n",
    "df_guess['lon_interp'] = np.nan\n",
    "for ac, group in testing.groupby('aircraft'): \n",
    "    t = group.timeAtServer.values\n",
    "    plon = np.poly1d(np.polyfit(t, group.longitude, 9))\n",
    "    plat = np.poly1d(np.polyfit(t, group.latitude, 9))\n",
    "    new_lons = plon(t)\n",
    "    new_lats = plat(t)\n",
    "    df_guess.loc[group.index, 'latitude'] = new_lats\n",
    "    df_guess.loc[group.index, 'longitude'] = new_lons\n",
    "df_guess.head()\n",
    "best_old = df_guess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions definitions for the multilateration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mod_z(val):\n",
    "    # Funciton to compute the modified z score\n",
    "    med = np.median(val)\n",
    "    med_abs_dev = np.median((np.abs(val - med)))\n",
    "    mod_z = 0.7413 * ((val - med) / med_abs_dev)\n",
    "    return np.abs(mod_z)\n",
    "\n",
    "\n",
    "def equation0102(p, *data):\n",
    "    group, triplet = data\n",
    "    id0, id1, id2 = triplet\n",
    "    c=0.2995 # Transmission speed\n",
    "    x, y = p\n",
    "    dt01 = group.loc[(group.s0==id0) & (group.s1==id1)].dt_corrected2.values[0]\n",
    "    dt02 = group.loc[(group.s0==id0) & (group.s1==id2)].dt_corrected2.values[0]\n",
    "    dt12 = group.loc[(group.s0==id1) & (group.s1==id2)].dt_corrected2.values[0]\n",
    "\n",
    "\n",
    "    s0 = sensors.loc[sensors.serial==id0][['latitude', 'longitude']].values\n",
    "    s1 = sensors.loc[sensors.serial==id1][['latitude', 'longitude']].values\n",
    "    s2 = sensors.loc[sensors.serial==id2][['latitude', 'longitude']].values\n",
    "\n",
    "\n",
    "    ap_alt = group.geoAltitude.values[0]\n",
    "    vert0 = ap_alt-sensors.loc[sensors.serial==id0].height.values[0]\n",
    "    vert1 = ap_alt-sensors.loc[sensors.serial==id1].height.values[0]\n",
    "    vert2 = ap_alt-sensors.loc[sensors.serial==id2].height.values[0]\n",
    "    return (\n",
    "            (np.sqrt(geopy.distance.distance(s0, (x, y)).m**2 + vert0**2)\n",
    "             -np.sqrt(geopy.distance.distance(s1, (x, y)).m**2 + vert1**2))/c-dt01,\n",
    "            (np.sqrt(geopy.distance.distance(s0, (x, y)).m**2 + vert0**2)\n",
    "             -np.sqrt(geopy.distance.distance(s2, (x, y)).m**2 + vert2**2))/c-dt02\n",
    "    )\n",
    "\n",
    "def equation0212(p, *data):\n",
    "    group, triplet = data\n",
    "    id0, id1, id2 = triplet\n",
    "    c=0.2995 # Transmission speed\n",
    "    x, y = p\n",
    "    dt01 = group.loc[(group.s0==id0) & (group.s1==id1)].dt_corrected2.values[0]\n",
    "    dt02 = group.loc[(group.s0==id0) & (group.s1==id2)].dt_corrected2.values[0]\n",
    "    dt12 = group.loc[(group.s0==id1) & (group.s1==id2)].dt_corrected2.values[0]\n",
    "\n",
    "\n",
    "    s0 = sensors.loc[sensors.serial==id0][['latitude', 'longitude']].values\n",
    "    s1 = sensors.loc[sensors.serial==id1][['latitude', 'longitude']].values\n",
    "    s2 = sensors.loc[sensors.serial==id2][['latitude', 'longitude']].values\n",
    "\n",
    "\n",
    "    ap_alt = group.geoAltitude.values[0]\n",
    "    vert0 = ap_alt-sensors.loc[sensors.serial==id0].height.values[0]\n",
    "    vert1 = ap_alt-sensors.loc[sensors.serial==id1].height.values[0]\n",
    "    vert2 = ap_alt-sensors.loc[sensors.serial==id2].height.values[0]\n",
    "    return (\n",
    "            (np.sqrt(geopy.distance.distance(s0, (x, y)).m**2 + vert0**2)\n",
    "             -np.sqrt(geopy.distance.distance(s2, (x, y)).m**2 + vert2**2))/c-dt02,\n",
    "            (np.sqrt(geopy.distance.distance(s1, (x, y)).m**2 + vert1**2)\n",
    "             -np.sqrt(geopy.distance.distance(s2, (x, y)).m**2 + vert2**2))/c-dt12\n",
    "    )\n",
    "\n",
    "def equation_all_in(p, group):\n",
    "    # This function is used to generate equations to solve using all the sensors availables\n",
    "    c = 0.2995 # Transmission speed\n",
    "    x, y = p\n",
    "    dts = []\n",
    "    ap_alt = group.geoAltitude.values[0]\n",
    "    list_sensors = sorted(set(group[['s0', 's1']].values.ravel())) # We retrieve all the sensors\n",
    "    sensor_pairs = [sorted(t) for t in list(combinations(list_sensors, 2))] # We generate all the pair of sensors\n",
    "    # We retrieve all the dt for each pair of sensor\n",
    "    for pair in sensor_pairs:\n",
    "        dts.append(group.loc[(group.s0==pair[0]) & (group.s1==pair[1])].dt_corrected2.values[0])\n",
    "\n",
    "    # We retrieve all the sensors positions as well as the vertical differences with the airplane\n",
    "    s_pos = [sensors.loc[sensors.serial==id0][['latitude', 'longitude']].values for id0 in list_sensors]\n",
    "    vert_diff = [ap_alt-sensors.loc[sensors.serial==id0].height.values[0]  for id0 in list_sensors]\n",
    "    # We append a list of equations with one eqation for each pair.\n",
    "    eq = []\n",
    "    for i, pair in enumerate(sensor_pairs):\n",
    "        index0, index1 = list_sensors.index(pair[0]), list_sensors.index(pair[1])\n",
    "        eq.append((np.sqrt(geopy.distance.distance(s_pos[index0], (x, y)).m**2 + vert_diff[index0]**2)\n",
    "                 -np.sqrt(geopy.distance.distance(s_pos[index1], (x, y)).m**2 + vert_diff[index1]**2))/c-dts[i])\n",
    "    return eq\n",
    "\n",
    "\n",
    "def apply_multilateration3(list_ids):\n",
    "    dico_res = {}\n",
    "    for ids in tqdm(list_ids):\n",
    "        group = X_test2.loc[X_test2.id == ids]\n",
    "        \n",
    "        # Knowing the location of 3 sensors and the dt between each pair, we can deduct the aircraft position\n",
    "        # Note that with 3 sensors, there are 2 possibilities of solution, hence why we have 2 equations\n",
    "        \n",
    "        if len(group)>1:\n",
    "            \n",
    "            # We generate all the triplet of sensors combinations for a specific measurement id\n",
    "            triplet_sensors = [sorted(x) for x in list(itertools.combinations(set(group.s0.tolist() + group.s1.tolist()), 3))]\n",
    "            lats, lons, trip = [], [], []\n",
    "            for triplet in triplet_sensors:\n",
    "                # We retrieve the best guess for this measurement\n",
    "                guess = tuple(best_old.loc[best_old.id==ids][['latitude', 'longitude']].values[0])\n",
    "                try:\n",
    "                    # We solve the aircraft position\n",
    "                    roots0102 =  fsolve(equation0102, guess, args=(group, triplet))             \n",
    "                    roots0212 =  fsolve(equation0212, guess, args=(group, triplet))\n",
    "                except:\n",
    "                    continue\n",
    "                \n",
    "                # If the difference with the guess is greater than 5deg we consider it's an outlier\n",
    "                error_max=5 \n",
    "                if np.abs(guess[0]-roots0102[0])<error_max and np.abs(guess[1]-roots0102[1])<error_max:\n",
    "                    lats.append(roots0102[0])\n",
    "                    lons.append(roots0102[1])\n",
    "                    trip.append(triplet)\n",
    "                if np.abs(guess[0]-roots0212[0])<error_max and np.abs(guess[1]-roots0212[1])<error_max:\n",
    "                    lats.append(roots0212[0])\n",
    "                    lons.append(roots0212[1])\n",
    "                    trip.append(triplet)\n",
    "        \n",
    "            # Our new guess becomes the median of all the results we have obtained\n",
    "            guess = np.median(lats), np.median(lons)\n",
    "            \n",
    "            # We store the obtained results in the dict of results\n",
    "            dico_res[ids] = (lats, lons, trip, np.nan, np.nan, np.nan, np.nan)\n",
    "            try:\n",
    "                # We transform the dict in a dataframe for easier manipulations and compute the z score for lats and lons\n",
    "                df = pd.DataFrame(np.column_stack(dico_res[ids][:3]), columns=['lats','lons', 's0', 's1', 's2'])\n",
    "                df['mod_z_lat'] = mod_z(lats)\n",
    "                df['mod_z_lon'] = mod_z(lons)\n",
    "                # We filter out all the measurements ehich have a z score greater than 3.5\n",
    "                sensors_filtered = df.loc[~((df.mod_z_lat>3.5) | (df.mod_z_lon>3.5))][['s0', 's1', 's2']].values.ravel()\n",
    "                # Then we retrieve the measurements of pairs of sensors which are in the filtered sensors\n",
    "                group_filtered = group.loc[(group.s0.isin(sensors_filtered)) & (group.s1.isin(sensors_filtered))]\n",
    "\n",
    "                # First we try to solve the multilateration problem on the filtered group\n",
    "                # We compute the solution using root method and store it in the dictionary\n",
    "                all_in_filt_sol = root(equation_all_in, guess, args=group_filtered, method='lm').x # Retrieve lat and lon solution of the set of equations\n",
    "                dico_res[ids] = (lats, lons, trip, all_in_filt_sol[0], all_in_filt_sol[1], np.nan, np.nan)\n",
    "                \n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            try:\n",
    "                # Then we try to solve the multilateration problem on the unfiltered group and store the result in the dict\n",
    "                all_in_sol = root(equation_all_in, guess, args=group, method='lm').x\n",
    "                dico_res[ids] = (lats, lons, trip, all_in_filt[0], all_in_filt[1], all_in_sol[0], all_in_sol[1])\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    return dico_res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open('dico_res_full_clean_v2_allin.pickle', 'rb') as handle:\n",
    "        dico_res = pickle.load(handle)\n",
    "except:\n",
    "    pass\n",
    "    global X_test2\n",
    "    X_test2 = X_test.copy()\n",
    "\n",
    "    list_ids = testing.id.values.tolist()\n",
    "    num_processes = mp.cpu_count()\n",
    "\n",
    "    # calculate the chunk size as an integer\n",
    "    chunk_size = int((len(list_ids)//num_processes)+1)\n",
    "\n",
    "    # Create the chunks\n",
    "    chunks = [list_ids[i:i + chunk_size]for i in range(0, len(list_ids), chunk_size)]\n",
    "\n",
    "    with mp.Pool(num_processes) as pool:\n",
    "        list_dicos = pool.map(apply_multilateration3, chunks)\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "    dico_res = {k: v for d in list_dicos for k, v in d.items()}\n",
    "\n",
    "#     with open('dico_res_full_clean_v2_allin.pickle', 'wb') as handle:\n",
    "#         pickle.dump(dico_res, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilateration Result analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.DataFrame.from_dict(dico_res, orient='index', columns=['lats', 'lons', 'triplet', 'lat_all_in_filt',\n",
    "                                                               'lon_all_in_filt', 'lat_all_in', 'lon_all_in'])\n",
    "res['lat_med'] = res.lats.apply(lambda x: np.median(x))\n",
    "res['lat_mean'] = res.lats.apply(lambda x: np.mean(x))\n",
    "res['lon_med'] = res.lons.apply(lambda x: np.median(x))\n",
    "res['lon_mean'] = res.lons.apply(lambda x: np.mean(x))\n",
    "res['n_res'] = res.lons.apply(lambda x: len(x))\n",
    "res['lats_iqr'] = res.lats.apply(lambda x: iqr(x))\n",
    "res['lons_iqr'] = res.lons.apply(lambda x: iqr(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering\n",
    "Heere we will only keep the results where er have at least 9 triplet of sensors giving a solution and when the IQR in below a certain treshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_res_min = 8\n",
    "lats_iqr_max = 0.0008151\n",
    "\n",
    "res = res.loc[(res.n_res>n_res_min) & (res.lats_iqr<lats_iqr_max)]\n",
    "res_merged = testing.set_index('id').join(res).dropna(subset=['lat_med'])\n",
    "res_merged.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we only keep the rows where the median of the triplet results is very close to the \"all in\" one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_merged = res_merged.loc[np.abs(res_merged.lat_med-res_merged.lat_all_in_filt)<100E-6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering and smoothing at trajectory level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we define utility functions. The first one will split trajectories into segments when there is no measurement for more than dt seconds. The second one will check that the speed of an airplane betwwen 2 measurements is possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import pchip_interpolate\n",
    "from scipy.interpolate import CubicSpline\n",
    "\n",
    "testing['latitude'] = np.nan\n",
    "testing['longitude'] = np.nan\n",
    "\n",
    "def split_times(t, dt=15):\n",
    "    # Split e time vector into chunks when the delat between 2 timesamps is greater than dt\n",
    "    diffs = [0]+ list(np.diff(t))\n",
    "    L = [ti[1] if diffs[ti[0]] <dt else 'split' for ti in enumerate(t)]\n",
    "    from itertools import groupby\n",
    "\n",
    "    # define separator keys\n",
    "    def split_condition(x):\n",
    "        return x in {'split'}\n",
    "\n",
    "    # define groupby object\n",
    "    grouper = groupby(L, key=split_condition)\n",
    "\n",
    "    # convert to dictionary via enumerate\n",
    "    return dict(enumerate((list(j) for i, j in grouper if not i), 1))\n",
    "\n",
    "\n",
    "def filter_speed(res_ac2, min_speed=50, max_speed=300):\n",
    "    # Check that the ground speed between 2 estimations is consistant. If not it removes the faulty one.\n",
    "    # Speeds are converted to m/s\n",
    "    lat_med = np.median(res_ac2.lat_all_in_filt)\n",
    "    lon_med = np.median(res_ac2.lon_all_in_filt)\n",
    "    # We convert a degree of lat and lon into meters\n",
    "    delta_lon_deg = geopy.distance.distance((lat_med, lon_med), (lat_med, lon_med+1)).m\n",
    "    delta_lat_deg = geopy.distance.distance((lat_med, lon_med), (lat_med+1, lon_med)).m\n",
    "    \n",
    "    speeds_lat = np.diff(res_ac2.lat_all_in_filt)*delta_lat_deg / np.diff(res_ac2.timeAtServer)\n",
    "    speeds_lon = np.diff(res_ac2.lon_all_in_filt)*delta_lon_deg / np.diff(res_ac2.timeAtServer)\n",
    "\n",
    "    # We check that the speeds are within the limits\n",
    "    speed_cond = (min_speed < np.sqrt(speeds_lat**2 + speeds_lon**2)) & (np.sqrt(speeds_lat**2 + speeds_lon**2)<max_speed)\n",
    "    \n",
    "    # We use recursion to remove outliers\n",
    "    # We retrieve the index of the faulty ones\n",
    "    index_faulty = [i for i, x in enumerate(speed_cond) if ~x]\n",
    "    if len(index_faulty) > 0:\n",
    "        # Could have been done much nicer ...\n",
    "        speed_cond = [True] * len(speed_cond) # We generate a dummy speed cond array\n",
    "        speed_cond[index_faulty[0]] = False # And we mark as faulty the first faulty one\n",
    "        res_ac2 = res_ac2.iloc[:-1].loc[speed_cond]\n",
    "    \n",
    "    if len(index_faulty)>1:\n",
    "        res_ac2 = filter_speed(res_ac2)\n",
    "        \n",
    "    return res_ac2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpt = 0\n",
    "for ac, group in testing.groupby('aircraft'):\n",
    "    fig = make_subplots(rows=1, cols=2)\n",
    "    # We retrieve the full time vector of the traj\n",
    "    time = testing.loc[testing.aircraft==ac].timeAtServer.values\n",
    "    \n",
    "    # We retrieve the part of the results that belongs to the aircraft traj\n",
    "    res_multilateration = res.loc[res.index.isin(group.id.values)]\n",
    "\n",
    "    if len(res_multilateration) > 0:\n",
    "        \n",
    "        res_ac = res_merged.loc[res_merged.aircraft==ac].copy()\n",
    "        res_ac = res_ac.groupby('timeAtServer', group_keys=False, as_index=False).apply(lambda x: x.loc[x.n_res.idxmax()])\n",
    "        res_ac = res_ac.sort_values(by=['timeAtServer'])\n",
    "\n",
    "\n",
    "        # Time vector of the points for which we have an estimate\n",
    "        t = res_ac.timeAtServer.values\n",
    "\n",
    "        # We split the traj in multiple segments if there is a gap of dt sec:\n",
    "        dico_t = split_times(t, dt=25.5)\n",
    "        \n",
    "        # Now we iterate each segment of the trajectory\n",
    "        for key in dico_t:\n",
    "            t = dico_t[key]\n",
    "            \n",
    "            # Very ugly trick to minimize the score based on visual impression\n",
    "            if ac == 149 and min(t) < 1200:\n",
    "                continue\n",
    "            if ac == 1429 and min(t) < 2800:\n",
    "                continue\n",
    "            \n",
    "            # We make sure that the trajector has at least 2 measurements:\n",
    "            if len(t) < 2:\n",
    "                continue\n",
    "                \n",
    "            res_ac = filter_speed(res_ac)      \n",
    "            res_ac_filtered = res_ac.loc[res_ac.timeAtServer.isin(t)]\n",
    "            if len(res_ac_filtered)<2:\n",
    "                continue\n",
    "            t = res_ac_filtered.timeAtServer.values\n",
    "            \n",
    "            # Interpolation between the first and lat timestamp of each chunk\n",
    "            time_inter = [tim for tim in time if np.min(t)<= tim <= np.max(t)]\n",
    "            new_lons = pchip_interpolate(t, res_ac_filtered.lon_all_in_filt, time_inter)\n",
    "            new_lats = pchip_interpolate(t, res_ac_filtered.lat_all_in_filt, time_inter)\n",
    "\n",
    "            # Smooting using splines ig there are more than 5 measurements in the segment\n",
    "            if len(new_lons) > 5:\n",
    "                spl = UnivariateSpline(time_inter, new_lons, k=5, s=0.000018)\n",
    "                new_lons = spl(time_inter)\n",
    "                spl = UnivariateSpline(time_inter, new_lats, k=5, s=0.000018)\n",
    "                new_lats = spl(time_inter)\n",
    "\n",
    "            \n",
    "            # We push the results into the testing dataframe\n",
    "            testing.loc[(testing.aircraft==ac) & (testing.timeAtServer.isin(time_inter)), 'latitude']  = new_lats\n",
    "            testing.loc[(testing.aircraft==ac) & (testing.timeAtServer.isin(time_inter)), 'longitude']  = new_lons\n",
    "            \n",
    "            # Some ploting\n",
    "            fig.add_trace(go.Scatter(x=t, y=res_ac_filtered.lat_all_in_filt, mode=\"markers\", name=\"lat_median\"), row=1, col=1)\n",
    "            fig.add_trace(go.Scatter(x=time_inter, y=new_lats, name=\"lat_interp\"),    row=1, col=1)\n",
    "            fig.add_trace(go.Scatter(x=t, y=res_ac_filtered.lon_all_in_filt, mode=\"markers\", name=\"lon_median\"), row=1, col=2)\n",
    "            fig.add_trace(go.Scatter(x=time_inter, y=new_lons, name=\"lon_interp\"), row=1, col=2)\n",
    "            \n",
    "            cpt += len(time_inter)\n",
    "            \n",
    "        fig.update_layout(title=str(ac))\n",
    "        fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('percentage:', cpt/len(testing)*100)\n",
    "# if cpt/len(testing) >= 0.5:\n",
    "#     print('saved')\n",
    "#     testing[['id', 'latitude', 'longitude', 'geoAltitude']].to_csv('test_31_07_{}_{}.csv'.format(n_res_min, lats_iqr_max),  index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}